{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q4. We will work with Part 1 folder in the lemm_stop folder. Show the code snippet to get marks for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How many documents are marked as spam and not spam? How did you come up with the number?\n",
    "2. How many words are there in all the documents?\n",
    "3. What are the top 5 frequent words in the spam documents?\n",
    "4. What are the top 5 frequent words in the non-spam documents?\n",
    "5. What is the maximum number of words in a document?\n",
    "6. What is the minimum number of words in a document?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of documents that're spams:  48\n",
      "No. of documents that're not spams:  241\n",
      "These are the spam documents:  ['spmsga114.txt', 'spmsga100.txt', 'spmsga128.txt', 'spmsga12.txt', 'spmsga13.txt', 'spmsga129.txt', 'spmsga101.txt', 'spmsga115.txt', 'spmsga103.txt', 'spmsga117.txt', 'spmsga11.txt', 'spmsga10.txt', 'spmsga116.txt', 'spmsga102.txt', 'spmsga106.txt', 'spmsga112.txt', 'spmsga14.txt', 'spmsga113.txt', 'spmsga107.txt', 'spmsga139.txt', 'spmsga111.txt', 'spmsga105.txt', 'spmsga104.txt', 'spmsga110.txt', 'spmsga138.txt', 'spmsga141.txt', 'spmsga140.txt', 'spmsga1.txt', 'spmsga135.txt', 'spmsga121.txt', 'spmsga109.txt', 'spmsga108.txt', 'spmsga120.txt', 'spmsga134.txt', 'spmsga122.txt', 'spmsga136.txt', 'spmsga137.txt', 'spmsga123.txt', 'spmsga127.txt', 'spmsga133.txt', 'spmsga132.txt', 'spmsga126.txt', 'spmsga118.txt', 'spmsga130.txt', 'spmsga124.txt', 'spmsga125.txt', 'spmsga131.txt', 'spmsga119.txt']\n"
     ]
    }
   ],
   "source": [
    "#4.1\n",
    "directory = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part1'\n",
    "import re\n",
    "import os\n",
    "\n",
    "emails = os.listdir(directory)\n",
    "keyword = \"spmsg\"\n",
    "spam_count = 0\n",
    "non_spam_count = 0\n",
    "spam_docs = []\n",
    "non_spam_docs = []\n",
    "for each_email in emails:\n",
    "    if keyword in each_email:\n",
    "        spam = each_email\n",
    "        spam_docs.append(spam)\n",
    "        spam_count = spam_count + 1\n",
    "    else:\n",
    "        non_spam = each_email\n",
    "        non_spam_docs.append(non_spam)\n",
    "        non_spam_count = non_spam_count + 1\n",
    "\n",
    "print(\"No. of documents that're spams: \", spam_count)\n",
    "print(\"No. of documents that're not spams: \", non_spam_count)\n",
    "print(\"These are the spam documents: \", spam_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.2\n",
    "from collections import Counter\n",
    "\n",
    "def create_dict(directory):\n",
    "    emails = [os.path.join(directory,f) for f in os.listdir(directory)]\n",
    "    all_words = []\n",
    "    for email in emails:\n",
    "        with open(email, 'r') as m: \n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2: \n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "    dictionary1 = Counter(all_words)\n",
    "    \n",
    "    #extracting only words from the dictionary\n",
    "    keys = dictionary1.keys()\n",
    "    for item in all_words:\n",
    "        if item.isalpha() == False: \n",
    "            del dictionary1[item]\n",
    "        elif len(item) == 1:\n",
    "            del dictionary1[item]\n",
    "    \n",
    "    return dictionary1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'first': 108,\n",
       "         'announcement': 11,\n",
       "         'groningen': 10,\n",
       "         'assembly': 2,\n",
       "         'language': 520,\n",
       "         'acquisition': 31,\n",
       "         'university': 296,\n",
       "         'netherland': 19,\n",
       "         'september': 21,\n",
       "         'conference': 73,\n",
       "         'aim': 10,\n",
       "         'bring': 21,\n",
       "         'together': 16,\n",
       "         'researcher': 9,\n",
       "         'discuss': 20,\n",
       "         'merit': 4,\n",
       "         'constraint': 15,\n",
       "         'different': 77,\n",
       "         'theoretical': 33,\n",
       "         'approach': 34,\n",
       "         'particular': 37,\n",
       "         'generative': 14,\n",
       "         'linguistics': 151,\n",
       "         'constructionism': 1,\n",
       "         'dynamic': 5,\n",
       "         'system': 59,\n",
       "         'model': 27,\n",
       "         'connectionism': 3,\n",
       "         'invite': 27,\n",
       "         'speaker': 54,\n",
       "         'harald': 2,\n",
       "         'clahsen': 2,\n",
       "         'essex': 3,\n",
       "         'annette': 1,\n",
       "         'karmiloff': 1,\n",
       "         'smith': 3,\n",
       "         'mrc': 1,\n",
       "         'cog': 4,\n",
       "         'dev': 1,\n",
       "         'unit': 9,\n",
       "         'london': 25,\n",
       "         'kim': 3,\n",
       "         'plunkett': 1,\n",
       "         'oxford': 10,\n",
       "         'luigus': 1,\n",
       "         'rizzus': 2,\n",
       "         'geneva': 2,\n",
       "         'paul': 9,\n",
       "         'van': 12,\n",
       "         'geert': 5,\n",
       "         'call': 76,\n",
       "         'papers': 47,\n",
       "         'preregistration': 1,\n",
       "         'appear': 29,\n",
       "         'february': 6,\n",
       "         'april': 14,\n",
       "         'abstract': 31,\n",
       "         'cover': 10,\n",
       "         'aspect': 29,\n",
       "         'relum': 1,\n",
       "         'ting': 1,\n",
       "         'core': 5,\n",
       "         'area': 71,\n",
       "         'include': 111,\n",
       "         'phonology': 32,\n",
       "         'morphology': 21,\n",
       "         'syntax': 47,\n",
       "         'semantics': 18,\n",
       "         'interface': 6,\n",
       "         'selection': 5,\n",
       "         'base': 50,\n",
       "         'quality': 8,\n",
       "         'potential': 15,\n",
       "         'contribute': 18,\n",
       "         'interactive': 2,\n",
       "         'objective': 1,\n",
       "         'further': 46,\n",
       "         'information': 204,\n",
       "         'charlotte': 3,\n",
       "         'koster': 1,\n",
       "         'frank': 2,\n",
       "         'wijnen': 1,\n",
       "         'gala': 1,\n",
       "         'coordinator': 4,\n",
       "         'dutch': 7,\n",
       "         'dept': 22,\n",
       "         'postbus': 1,\n",
       "         'let': 40,\n",
       "         'rug': 8,\n",
       "         'nl': 9,\n",
       "         'fax': 112,\n",
       "         'pardon': 3,\n",
       "         'intrusion': 7,\n",
       "         'offence': 1,\n",
       "         'mean': 51,\n",
       "         'interest': 143,\n",
       "         'simply': 27,\n",
       "         'ignore': 7,\n",
       "         'delete': 14,\n",
       "         'message': 46,\n",
       "         'hello': 5,\n",
       "         'honestly': 1,\n",
       "         'believe': 34,\n",
       "         'lifeline': 1,\n",
       "         'guarantee': 19,\n",
       "         'pull': 2,\n",
       "         'debt': 16,\n",
       "         'throw': 7,\n",
       "         'please': 152,\n",
       "         'grab': 2,\n",
       "         'simple': 32,\n",
       "         'inexpensive': 2,\n",
       "         'solution': 7,\n",
       "         'problem': 65,\n",
       "         'one': 290,\n",
       "         'program': 150,\n",
       "         'enable': 3,\n",
       "         'obtain': 11,\n",
       "         'secure': 2,\n",
       "         'financial': 15,\n",
       "         'freedom': 9,\n",
       "         'nobrainer': 1,\n",
       "         'compound': 14,\n",
       "         'leverage': 1,\n",
       "         'educational': 4,\n",
       "         'entitle': 5,\n",
       "         'life': 33,\n",
       "         'without': 41,\n",
       "         'best': 47,\n",
       "         'part': 45,\n",
       "         'cost': 42,\n",
       "         'begin': 29,\n",
       "         'few': 42,\n",
       "         'week': 56,\n",
       "         'ago': 33,\n",
       "         'respond': 13,\n",
       "         'letter': 59,\n",
       "         'send': 166,\n",
       "         'start': 53,\n",
       "         'registration': 35,\n",
       "         'process': 35,\n",
       "         'show': 70,\n",
       "         'weekly': 10,\n",
       "         'convert': 1,\n",
       "         'later': 17,\n",
       "         'achieve': 6,\n",
       "         'complete': 45,\n",
       "         'independence': 4,\n",
       "         'participate': 6,\n",
       "         'main': 11,\n",
       "         'plan': 39,\n",
       "         'net': 20,\n",
       "         'control': 20,\n",
       "         'finances': 1,\n",
       "         'especially': 27,\n",
       "         'mortgage': 14,\n",
       "         'note': 56,\n",
       "         'credit': 98,\n",
       "         'card': 61,\n",
       "         'solid': 3,\n",
       "         'old': 32,\n",
       "         'free': 75,\n",
       "         'company': 36,\n",
       "         'establish': 22,\n",
       "         'customer': 9,\n",
       "         'over': 105,\n",
       "         'receive': 120,\n",
       "         'detail': 38,\n",
       "         'join': 18,\n",
       "         'us': 120,\n",
       "         'our': 190,\n",
       "         'tuesday': 8,\n",
       "         'night': 20,\n",
       "         'pm': 7,\n",
       "         'cst': 1,\n",
       "         'telephone': 11,\n",
       "         'number': 107,\n",
       "         'peace': 1,\n",
       "         'mind': 12,\n",
       "         'worth': 14,\n",
       "         'today': 46,\n",
       "         'print': 22,\n",
       "         'form': 123,\n",
       "         'return': 15,\n",
       "         'entire': 9,\n",
       "         'cash': 20,\n",
       "         'computer': 48,\n",
       "         'center': 20,\n",
       "         'po': 13,\n",
       "         'box': 28,\n",
       "         'grapevine': 1,\n",
       "         'texa': 11,\n",
       "         'member': 55,\n",
       "         'reside': 1,\n",
       "         'outside': 18,\n",
       "         'usa': 19,\n",
       "         'must': 60,\n",
       "         'mailer': 9,\n",
       "         'id': 4,\n",
       "         'name': 126,\n",
       "         'address': 200,\n",
       "         'city': 33,\n",
       "         'state': 82,\n",
       "         'zip': 19,\n",
       "         'phone': 81,\n",
       "         'want': 102,\n",
       "         'hunt': 3,\n",
       "         'camp': 3,\n",
       "         'vacation': 3,\n",
       "         'felton': 1,\n",
       "         'wild': 1,\n",
       "         'wonderful': 2,\n",
       "         'west': 14,\n",
       "         'virginium': 6,\n",
       "         'per': 60,\n",
       "         'day': 79,\n",
       "         'pay': 58,\n",
       "         'room': 28,\n",
       "         'three': 39,\n",
       "         'home': 53,\n",
       "         'cook': 1,\n",
       "         'meal': 2,\n",
       "         'pack': 3,\n",
       "         'lunch': 15,\n",
       "         'stay': 8,\n",
       "         'wood': 1,\n",
       "         'noon': 1,\n",
       "         'cozy': 1,\n",
       "         'accomodation': 3,\n",
       "         'reserve': 8,\n",
       "         'space': 8,\n",
       "         'follow': 103,\n",
       "         'season': 3,\n",
       "         'book': 98,\n",
       "         'buck': 2,\n",
       "         'nov': 3,\n",
       "         'dec': 5,\n",
       "         'doe': 24,\n",
       "         'announce': 10,\n",
       "         'muzzel': 1,\n",
       "         'loader': 1,\n",
       "         'deer': 2,\n",
       "         'archery': 1,\n",
       "         'oct': 2,\n",
       "         'turkey': 1,\n",
       "         'sesson': 1,\n",
       "         'mail': 153,\n",
       "         'compuserve': 6,\n",
       "         'com': 42,\n",
       "         'anybody': 19,\n",
       "         'recent': 32,\n",
       "         'work': 232,\n",
       "         'uninflect': 1,\n",
       "         'tag': 2,\n",
       "         'huh': 2,\n",
       "         'right': 42,\n",
       "         'example': 80,\n",
       "         'below': 41,\n",
       "         're': 37,\n",
       "         'replace': 8,\n",
       "         'dos': 1,\n",
       "         'job': 42,\n",
       "         'contact': 62,\n",
       "         'directly': 25,\n",
       "         'll': 32,\n",
       "         'summarize': 6,\n",
       "         'response': 69,\n",
       "         'thank': 83,\n",
       "         'neal': 2,\n",
       "         'norrick': 1,\n",
       "         'niu': 3,\n",
       "         'bitnet': 19,\n",
       "         'has': 9,\n",
       "         'else': 28,\n",
       "         'weird': 3,\n",
       "         'piece': 8,\n",
       "         'newspaper': 9,\n",
       "         'read': 54,\n",
       "         'sunday': 5,\n",
       "         'morn': 10,\n",
       "         'local': 20,\n",
       "         'gannett': 1,\n",
       "         'paper': 75,\n",
       "         'promptly': 1,\n",
       "         'misplace': 1,\n",
       "         'measure': 9,\n",
       "         'greek': 23,\n",
       "         'mayor': 6,\n",
       "         'small': 23,\n",
       "         'massachusett': 15,\n",
       "         'town': 12,\n",
       "         'ethnically': 2,\n",
       "         'mix': 6,\n",
       "         'population': 7,\n",
       "         'prevent': 6,\n",
       "         'foreign': 28,\n",
       "         'accent': 46,\n",
       "         'employ': 7,\n",
       "         'grade': 13,\n",
       "         'school': 76,\n",
       "         'teacher': 48,\n",
       "         'beside': 8,\n",
       "         'kill': 2,\n",
       "         'own': 63,\n",
       "         'chance': 5,\n",
       "         'ever': 37,\n",
       "         'teach': 71,\n",
       "         'nonsensical': 3,\n",
       "         'linguistically': 2,\n",
       "         'am': 3,\n",
       "         'assume': 17,\n",
       "         'child': 67,\n",
       "         'typically': 2,\n",
       "         'infect': 1,\n",
       "         'exploit': 5,\n",
       "         'bilingual': 6,\n",
       "         'daughter': 2,\n",
       "         'already': 18,\n",
       "         'mention': 41,\n",
       "         'list': 137,\n",
       "         'around': 29,\n",
       "         'experience': 62,\n",
       "         'english': 164,\n",
       "         'two': 83,\n",
       "         'native': 24,\n",
       "         'pertain': 3,\n",
       "         'query': 32,\n",
       "         'exposure': 4,\n",
       "         'primarily': 7,\n",
       "         'wife': 13,\n",
       "         'housekeeper': 1,\n",
       "         'nanny': 1,\n",
       "         'speak': 78,\n",
       "         'rather': 50,\n",
       "         'radical': 4,\n",
       "         'upper': 2,\n",
       "         'peninsulum': 1,\n",
       "         'michigan': 4,\n",
       "         'dialect': 40,\n",
       "         'warsh': 1,\n",
       "         'wash': 1,\n",
       "         'etc': 65,\n",
       "         'sarah': 3,\n",
       "         'never': 38,\n",
       "         'acquire': 16,\n",
       "         'refocus': 1,\n",
       "         'possible': 45,\n",
       "         'influence': 18,\n",
       "         'victor': 8,\n",
       "         'raskin': 11,\n",
       "         'cc': 8,\n",
       "         'purdue': 7,\n",
       "         'edu': 111,\n",
       "         'professor': 21,\n",
       "         'linguistic': 234,\n",
       "         'chair': 15,\n",
       "         'interdepartmental': 5,\n",
       "         'natural': 29,\n",
       "         'laboratory': 8,\n",
       "         'lafayette': 2,\n",
       "         'comply': 3,\n",
       "         'propose': 21,\n",
       "         'unite': 13,\n",
       "         'federal': 22,\n",
       "         'requirement': 15,\n",
       "         'commercial': 9,\n",
       "         'email': 216,\n",
       "         'bill': 25,\n",
       "         'section': 14,\n",
       "         'additional': 22,\n",
       "         'info': 10,\n",
       "         'http': 60,\n",
       "         'thoma': 7,\n",
       "         'loc': 1,\n",
       "         'gov': 7,\n",
       "         'search': 25,\n",
       "         'require': 24,\n",
       "         'sender': 7,\n",
       "         'network': 10,\n",
       "         'internet': 49,\n",
       "         'tool': 23,\n",
       "         'waterford': 1,\n",
       "         'wi': 2,\n",
       "         'paragraph': 5,\n",
       "         'transmission': 2,\n",
       "         'stop': 26,\n",
       "         'reply': 39,\n",
       "         'subject': 63,\n",
       "         'remove': 44,\n",
       "         'dear': 16,\n",
       "         'fellow': 5,\n",
       "         'entrepreneur': 4,\n",
       "         'post': 51,\n",
       "         'presence': 2,\n",
       "         'certain': 20,\n",
       "         'web': 36,\n",
       "         'lead': 14,\n",
       "         'promote': 3,\n",
       "         'product': 55,\n",
       "         'service': 40,\n",
       "         'need': 116,\n",
       "         'proper': 9,\n",
       "         'fingertip': 4,\n",
       "         'power': 17,\n",
       "         'reach': 13,\n",
       "         'ten': 10,\n",
       "         'million': 36,\n",
       "         'advantage': 11,\n",
       "         'unwise': 1,\n",
       "         'software': 45,\n",
       "         'assist': 5,\n",
       "         'online': 14,\n",
       "         'business': 56,\n",
       "         'venture': 2,\n",
       "         'unbelievable': 4,\n",
       "         'level': 64,\n",
       "         'success': 28,\n",
       "         'target': 7,\n",
       "         'vast': 3,\n",
       "         'audience': 5,\n",
       "         'much': 99,\n",
       "         'set': 35,\n",
       "         'storefront': 1,\n",
       "         'item': 12,\n",
       "         'help': 78,\n",
       "         'try': 30,\n",
       "         'evaluation': 11,\n",
       "         'basis': 21,\n",
       "         'luck': 4,\n",
       "         'regard': 22,\n",
       "         'sale': 21,\n",
       "         'request': 32,\n",
       "         'wish': 25,\n",
       "         'ooo': 2,\n",
       "         'permanently': 3,\n",
       "         'future': 17,\n",
       "         'mailing': 12,\n",
       "         'horizon': 1,\n",
       "         'publish': 60,\n",
       "         'saltwater': 1,\n",
       "         'aquarium': 1,\n",
       "         'hobbyist': 1,\n",
       "         'website': 8,\n",
       "         'feature': 19,\n",
       "         'fish': 2,\n",
       "         'dictionary': 26,\n",
       "         'nationwide': 1,\n",
       "         'shop': 10,\n",
       "         'listing': 2,\n",
       "         'manufacturer': 1,\n",
       "         'link': 21,\n",
       "         'review': 24,\n",
       "         'www': 39,\n",
       "         'captivesea': 1,\n",
       "         'wald': 3,\n",
       "         'grosserhode': 1,\n",
       "         'idea': 30,\n",
       "         'connection': 15,\n",
       "         'between': 59,\n",
       "         'imperialism': 5,\n",
       "         'norwegian': 3,\n",
       "         'linguist': 137,\n",
       "         'han': 5,\n",
       "         'vogt': 2,\n",
       "         'yes': 18,\n",
       "         'specialist': 9,\n",
       "         'kartvelian': 2,\n",
       "         'thing': 55,\n",
       "         'leaflet': 1,\n",
       "         'maalstrid': 1,\n",
       "         'og': 1,\n",
       "         'klassekamp': 1,\n",
       "         'debate': 8,\n",
       "         'class': 43,\n",
       "         'struggle': 4,\n",
       "         'brief': 5,\n",
       "         'reference': 63,\n",
       "         'article': 27,\n",
       "         'research': 75,\n",
       "         'policy': 8,\n",
       "         'ammon': 1,\n",
       "         'dittmar': 1,\n",
       "         'mattheier': 2,\n",
       "         'ed': 23,\n",
       "         'sociolinguist': 2,\n",
       "         'international': 49,\n",
       "         'handbook': 4,\n",
       "         'berlin': 10,\n",
       "         'explicitly': 3,\n",
       "         'oppose': 10,\n",
       "         'view': 35,\n",
       "         'geoffrey': 4,\n",
       "         'sampson': 1,\n",
       "         'refer': 22,\n",
       "         'though': 36,\n",
       "         'hartmut': 2,\n",
       "         'haberland': 1,\n",
       "         'ye': 13,\n",
       "         'indeed': 10,\n",
       "         'careful': 4,\n",
       "         'star': 14,\n",
       "         'sentence': 23,\n",
       "         'sentencing': 1,\n",
       "         'passerby': 1,\n",
       "         'even': 97,\n",
       "         'brentwood': 1,\n",
       "         'several': 30,\n",
       "         'witness': 5,\n",
       "         'cruel': 1,\n",
       "         'double': 8,\n",
       "         'murder': 1,\n",
       "         'testimony': 2,\n",
       "         'break': 25,\n",
       "         'professional': 19,\n",
       "         'interpret': 9,\n",
       "         'latter': 9,\n",
       "         'sure': 43,\n",
       "         'once': 24,\n",
       "         'simpson': 1,\n",
       "         'result': 46,\n",
       "         'history': 23,\n",
       "         'jule': 2,\n",
       "         'levin': 1,\n",
       "         'south': 18,\n",
       "         'asian': 11,\n",
       "         'study': 93,\n",
       "         'hawaus': 7,\n",
       "         'ian': 4,\n",
       "         'pacific': 22,\n",
       "         'eleventh': 1,\n",
       "         'annual': 6,\n",
       "         'spring': 4,\n",
       "         'symposium': 6,\n",
       "         'prehistory': 4,\n",
       "         'asia': 2,\n",
       "         'hold': 36,\n",
       "         'march': 10,\n",
       "         'monday': 6,\n",
       "         'manoa': 3,\n",
       "         'campus': 22,\n",
       "         'thirty': 3,\n",
       "         'minute': 29,\n",
       "         'length': 12,\n",
       "         'focus': 18,\n",
       "         'structure': 45,\n",
       "         'modern': 40,\n",
       "         'classical': 8,\n",
       "         'asium': 6,\n",
       "         'afghanistan': 2,\n",
       "         'bangladesh': 1,\n",
       "         'bhutan': 1,\n",
       "         'indium': 2,\n",
       "         'maldive': 1,\n",
       "         'pakistan': 1,\n",
       "         'sikkim': 1,\n",
       "         'srus': 1,\n",
       "         'lanka': 1,\n",
       "         'tibet': 1,\n",
       "         'relationship': 10,\n",
       "         'among': 17,\n",
       "         'mainland': 1,\n",
       "         'insular': 2,\n",
       "         'east': 7,\n",
       "         'central': 10,\n",
       "         'western': 14,\n",
       "         'africa': 3,\n",
       "         'island': 9,\n",
       "         'fijus': 1,\n",
       "         'proceedings': 6,\n",
       "         'summer': 9,\n",
       "         'fall': 18,\n",
       "         'copy': 49,\n",
       "         'page': 25,\n",
       "         'anonymous': 7,\n",
       "         'attention': 17,\n",
       "         'karina': 1,\n",
       "         'bingham': 1,\n",
       "         'moore': 2,\n",
       "         'hall': 18,\n",
       "         'honolulu': 3,\n",
       "         'hi': 9,\n",
       "         'dr': 65,\n",
       "         'lawrence': 4,\n",
       "         'reid': 4,\n",
       "         'uhunix': 3,\n",
       "         'uhcc': 4,\n",
       "         'hawaius': 5,\n",
       "         'tendency': 6,\n",
       "         'both': 67,\n",
       "         'field': 51,\n",
       "         'special': 65,\n",
       "         'failure': 2,\n",
       "         'cite': 24,\n",
       "         'paradigm': 3,\n",
       "         'general': 56,\n",
       "         'stem': 6,\n",
       "         'social': 26,\n",
       "         'nature': 22,\n",
       "         'academic': 38,\n",
       "         'discourse': 35,\n",
       "         'society': 25,\n",
       "         'common': 19,\n",
       "         'biology': 4,\n",
       "         'physics': 15,\n",
       "         'deconstructionist': 2,\n",
       "         'theology': 1,\n",
       "         'matter': 30,\n",
       "         'worry': 10,\n",
       "         'happy': 15,\n",
       "         'sapir': 11,\n",
       "         'baudouin': 2,\n",
       "         'de': 253,\n",
       "         'courtenay': 2,\n",
       "         'current': 30,\n",
       "         'favorite': 7,\n",
       "         'choose': 15,\n",
       "         'clearly': 25,\n",
       "         'dan': 15,\n",
       "         'everett': 5,\n",
       "         'comment': 29,\n",
       "         'dissertation': 20,\n",
       "         've': 37,\n",
       "         'point': 64,\n",
       "         'implication': 3,\n",
       "         'fit': 10,\n",
       "         'antecedent': 3,\n",
       "         'mislead': 2,\n",
       "         'reason': 36,\n",
       "         'allude': 1,\n",
       "         'fact': 38,\n",
       "         'mit': 5,\n",
       "         'version': 20,\n",
       "         'circulate': 4,\n",
       "         'iulc': 1,\n",
       "         'garland': 1,\n",
       "         'usually': 14,\n",
       "         'acknowledgement': 4,\n",
       "         'tone': 8,\n",
       "         'leben': 1,\n",
       "         'true': 26,\n",
       "         'itself': 11,\n",
       "         'chapter': 6,\n",
       "         'entirely': 6,\n",
       "         'devote': 7,\n",
       "         'proposition': 1,\n",
       "         'continuation': 1,\n",
       "         'discussion': 43,\n",
       "         'american': 53,\n",
       "         'since': 53,\n",
       "         'pike': 13,\n",
       "         'most': 109,\n",
       "         'important': 30,\n",
       "         'theoretician': 4,\n",
       "         'period': 7,\n",
       "         'write': 81,\n",
       "         'bloch': 3,\n",
       "         'harri': 53,\n",
       "         'hockett': 4,\n",
       "         'style': 11,\n",
       "         'taste': 3,\n",
       "         'anything': 24,\n",
       "         'same': 53,\n",
       "         'theme': 10,\n",
       "         'perhap': 23,\n",
       "         'journal': 33,\n",
       "         'genealogical': 1,\n",
       "         'prosodic': 7,\n",
       "         'firthian': 2,\n",
       "         'autosegemmental': 1,\n",
       "         'major': 36,\n",
       "         'contributor': 6,\n",
       "         'during': 13,\n",
       "         'still': 44,\n",
       "         'alive': 1,\n",
       "         'intellectual': 10,\n",
       "         'active': 4,\n",
       "         'feel': 24,\n",
       "         'slight': 2,\n",
       "         'lack': 5,\n",
       "         'citation': 28,\n",
       "         'suggest': 33,\n",
       "         'indictment': 1,\n",
       "         'normal': 9,\n",
       "         'human': 37,\n",
       "         'expectation': 3,\n",
       "         'courtesy': 1,\n",
       "         'actually': 21,\n",
       "         'forget': 7,\n",
       "         'phonologist': 4,\n",
       "         'less': 43,\n",
       "         'sin': 3,\n",
       "         'many': 147,\n",
       "         'geoff': 4,\n",
       "         'huck': 1,\n",
       "         'relation': 20,\n",
       "         'semantic': 20,\n",
       "         'syntactic': 17,\n",
       "         'theory': 93,\n",
       "         'however': 40,\n",
       "         'again': 39,\n",
       "         'purely': 5,\n",
       "         'wonder': 14,\n",
       "         'myself': 24,\n",
       "         'material': 24,\n",
       "         'nonlinear': 1,\n",
       "         'contributer': 1,\n",
       "         'literature': 41,\n",
       "         'galvanize': 1,\n",
       "         'drop': 7,\n",
       "         'opportunity': 40,\n",
       "         'person': 40,\n",
       "         'absolutely': 7,\n",
       "         'delight': 1,\n",
       "         'bite': 10,\n",
       "         'mild': 2,\n",
       "         'reproof': 1,\n",
       "         'perceive': 7,\n",
       "         'leave': 26,\n",
       "         'anyone': 70,\n",
       "         'john': 40,\n",
       "         'goldsmith': 2,\n",
       "         'those': 77,\n",
       "         'linguists': 4,\n",
       "         'aw': 1,\n",
       "         'ay': 1,\n",
       "         'something': 24,\n",
       "         'word': 147,\n",
       "         'type': 59,\n",
       "         'vowel': 29,\n",
       "         'syllable': 17,\n",
       "         'writer': 16,\n",
       "         'second': 46,\n",
       "         'typewriter': 5,\n",
       "         'put': 32,\n",
       "         'another': 42,\n",
       "         'grammar': 82,\n",
       "         'negative': 11,\n",
       "         'concord': 4,\n",
       "         'various': 30,\n",
       "         'british': 12,\n",
       "         'natively': 1,\n",
       "         'answer': 27,\n",
       "         'grammaticality': 7,\n",
       "         'question': 84,\n",
       "         'effect': 10,\n",
       "         'back': 36,\n",
       "         'kroch': 1,\n",
       "         'change': 37,\n",
       "         'ling': 13,\n",
       "         'upenn': 5,\n",
       "         'hand': 18,\n",
       "         'frustrate': 3,\n",
       "         'vastly': 1,\n",
       "         'incorrect': 5,\n",
       "         'concern': 49,\n",
       "         'formalize': 3,\n",
       "         'symbol': 4,\n",
       "         'issue': 73,\n",
       "         'why': 62,\n",
       "         'rule': 62,\n",
       "         'baffle': 1,\n",
       "         'description': 23,\n",
       "         'grad': 3,\n",
       "         'student': 99,\n",
       "         'fluent': 9,\n",
       "         'contemporary': 2,\n",
       "         'comfortable': 2,\n",
       "         'large': 19,\n",
       "         'amount': 28,\n",
       "         'datum': 69,\n",
       "         'sometime': 11,\n",
       "         'often': 26,\n",
       "         'learn': 60,\n",
       "         'end': 21,\n",
       "         'kind': 41,\n",
       "         'knowledge': 28,\n",
       "         'whom': 5,\n",
       "         'analysis': 63,\n",
       "         'high': 26,\n",
       "         'priority': 8,\n",
       "         'readily': 2,\n",
       "         'translate': 6,\n",
       "         'bit': 5,\n",
       "         'conversation': 6,\n",
       "         'really': 34,\n",
       "         'task': 7,\n",
       "         'simultaneous': 1,\n",
       "         'interpreter': 6,\n",
       "         'someone': 31,\n",
       "         'probably': 16,\n",
       "         'tell': 54,\n",
       "         'structural': 15,\n",
       "         'obvious': 7,\n",
       "         'rattle': 1,\n",
       "         'head': 10,\n",
       "         'after': 37,\n",
       "         'couple': 12,\n",
       "         'where': 80,\n",
       "         'meet': 29,\n",
       "         'great': 30,\n",
       "         'little': 68,\n",
       "         'descrip': 1,\n",
       "         'tivist': 1,\n",
       "         'care': 12,\n",
       "         'development': 20,\n",
       "         'healthy': 3,\n",
       "         'seem': 74,\n",
       "         'category': 16,\n",
       "         'collector': 5,\n",
       "         'raw': 4,\n",
       "         'statistician': 5,\n",
       "         'analogy': 15,\n",
       "         'offer': 59,\n",
       "         'thought': 4,\n",
       "         'kathleen': 3,\n",
       "         'hubbard': 1,\n",
       "         'berkeley': 12,\n",
       "         'is': 54,\n",
       "         'department': 117,\n",
       "         'education': 22,\n",
       "         'univ': 11,\n",
       "         'austin': 8,\n",
       "         'availability': 2,\n",
       "         'certification': 1,\n",
       "         'certainly': 15,\n",
       "         'appreciate': 17,\n",
       "         'clue': 2,\n",
       "         'brett': 3,\n",
       "         'rosenberg': 3,\n",
       "         'arizona': 15,\n",
       "         'spanish': 34,\n",
       "         'portuguese': 4,\n",
       "         'cognitive': 30,\n",
       "         'july': 28,\n",
       "         'albuquerque': 4,\n",
       "         'mexico': 19,\n",
       "         'scope': 6,\n",
       "         'forum': 2,\n",
       "         'within': 56,\n",
       "         'perspective': 17,\n",
       "         'subsume': 2,\n",
       "         'broadly': 4,\n",
       "         'compatible': 3,\n",
       "         'share': 14,\n",
       "         'integral': 1,\n",
       "         'cognition': 3,\n",
       "         'reflect': 12,\n",
       "         'interaction': 13,\n",
       "         'cultural': 8,\n",
       "         'psychological': 3,\n",
       "         'communicative': 7,\n",
       "         'functional': 7,\n",
       "         'consideration': 5,\n",
       "         'understand': 37,\n",
       "         'context': 18,\n",
       "         'realistic': 1,\n",
       "         'conceptualization': 1,\n",
       "         'mental': 5,\n",
       "         'topic': 31,\n",
       "         'characteristic': 7,\n",
       "         'categorization': 2,\n",
       "         'prototypicality': 1,\n",
       "         'metaphor': 3,\n",
       "         'imagery': 1,\n",
       "         'principle': 31,\n",
       "         'organization': 7,\n",
       "         'iconicity': 1,\n",
       "         'naturalness': 1,\n",
       "         'conceptual': 4,\n",
       "         'experiential': 2,\n",
       "         'pragmatic': 15,\n",
       "         'background': 8,\n",
       "         'addition': 25,\n",
       "         'sign': 15,\n",
       "         'site': 37,\n",
       "         'place': 48,\n",
       "         'unm': 3,\n",
       "         'lie': 4,\n",
       "         'foothill': 1,\n",
       "         'sandium': 2,\n",
       "         'mountain': 3,\n",
       "         'peak': 1,\n",
       "         'foot': 5,\n",
       "         'mesa': 4,\n",
       "         'dot': 4,\n",
       "         'cinder': 1,\n",
       "         'cone': 1,\n",
       "         'volcano': 1,\n",
       "         'divide': 1,\n",
       "         'wind': 2,\n",
       "         'rio': 10,\n",
       "         'grande': 1,\n",
       "         'valley': 2,\n",
       "         'famous': 3,\n",
       "         'bosque': 1,\n",
       "         'preserve': 3,\n",
       "         'altitude': 1,\n",
       "         'roughly': 5,\n",
       "         'enjoy': 4,\n",
       "         'warm': 1,\n",
       "         'cool': 1,\n",
       "         'excursion': 2,\n",
       "         'nearby': 2,\n",
       "         'pueblo': 3,\n",
       "         'santa': 4,\n",
       "         'fe': 2,\n",
       "         'america': 14,\n",
       "         'biennial': 1,\n",
       "         'institute': 19,\n",
       "         'under': 24,\n",
       "         'direction': 11,\n",
       "         'joan': 1,\n",
       "         'bybee': 1,\n",
       "         'run': 32,\n",
       "         'six': 9,\n",
       "         'late': 19,\n",
       "         'june': 12,\n",
       "         'early': 20,\n",
       "         'august': 6,\n",
       "         'course': 65,\n",
       "         'visit': 13,\n",
       "         'faculty': 23,\n",
       "         'lecture': 16,\n",
       "         'apply': 69,\n",
       "         'comparison': 12,\n",
       "         'orientation': 4,\n",
       "         'emphasis': 4,\n",
       "         'precede': 5,\n",
       "         'icla': 1,\n",
       "         'submission': 22,\n",
       "         'author': 19,\n",
       "         'submit': 14,\n",
       "         'four': 14,\n",
       "         'hardcopy': 4,\n",
       "         'format': 20,\n",
       "         'before': 51,\n",
       "         'november': 12,\n",
       "         'notify': 1,\n",
       "         'acceptance': 5,\n",
       "         'rejection': 4,\n",
       "         'sherman': 1,\n",
       "         'wilcox': 1,\n",
       "         'nm': 1,\n",
       "         'ii': 14,\n",
       "         'jornadas': 2,\n",
       "         'linguistica': 26,\n",
       "         'aborigen': 6,\n",
       "         'bueno': 5,\n",
       "         'aire': 5,\n",
       "         'argentina': 15,\n",
       "         'noviembre': 5,\n",
       "         'facultad': 4,\n",
       "         'filosofia': 1,\n",
       "         'letras': 1,\n",
       "         'instituto': 7,\n",
       "         'comision': 18,\n",
       "         'organizadora': 1,\n",
       "         'ana': 18,\n",
       "         'gerzenstein': 7,\n",
       "         'fernandez': 11,\n",
       "         'garay': 4,\n",
       "         'lucium': 4,\n",
       "         'golluscio': 4,\n",
       "         'pedro': 7,\n",
       "         'viega': 4,\n",
       "         'barro': 5,\n",
       "         'yolanda': 4,\n",
       "         'gutierrez': 1,\n",
       "         'secretarium': 1,\n",
       "         'susana': 3,\n",
       "         'andreotta': 1,\n",
       "         'programa': 3,\n",
       "         'actividades': 2,\n",
       "         'academicas': 1,\n",
       "         'seminario': 1,\n",
       "         'doctorado': 1,\n",
       "         'christo': 3,\n",
       "         'clairi': 3,\n",
       "         'sintaxi': 2,\n",
       "         'funcional': 1,\n",
       "         'con': 6,\n",
       "         'aplicacion': 1,\n",
       "         'la': 86,\n",
       "         'lengua': 20,\n",
       "         'indoamericana': 1,\n",
       "         'curso': 1,\n",
       "         'rodolfo': 5,\n",
       "         'cerron': 3,\n",
       "         'palomino': 3,\n",
       "         'politica': 2,\n",
       "         'las': 5,\n",
       "         'jornada': 5,\n",
       "         'mart': 3,\n",
       "         'acreditacion': 1,\n",
       "         'apertura': 1,\n",
       "         'trabajo': 6,\n",
       "         'en': 66,\n",
       "         'comisiones': 6,\n",
       "         'descriptiva': 5,\n",
       "         'censabella': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part1'\n",
    "create_dict(directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are total  9963 words in all the documents.\n"
     ]
    }
   ],
   "source": [
    "directory = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part1'\n",
    "words = create_dict(directory)\n",
    "c = 0\n",
    "for key in words:\n",
    "    c = c+1\n",
    "print(\"There are total \", c, \"words in all the documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('email', 177),\n",
       " ('order', 156),\n",
       " ('address', 142),\n",
       " ('report', 128),\n",
       " ('mail', 122)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.3\n",
    "#finding top 5 frequent words in spam documents\n",
    "\n",
    "emails = [os.path.join(directory,f) for f in os.listdir(directory)]\n",
    "keyword=\"spmsga\"\n",
    "all_words = []\n",
    "for email in emails:\n",
    "    if keyword in email:\n",
    "        with open(email, 'r') as m: \n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2: \n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "dictionary2 = Counter(all_words)\n",
    "    \n",
    "    #extracting only words from the dictionary\n",
    "keys = dictionary2.keys()\n",
    "for item in all_words:\n",
    "    if item.isalpha() == False: \n",
    "        del dictionary2[item]\n",
    "    elif len(item) == 1:\n",
    "        del dictionary2[item]\n",
    "dictionary2 = dictionary2.most_common(5)\n",
    "dictionary2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('language', 520),\n",
       " ('university', 294),\n",
       " ('de', 253),\n",
       " ('linguistic', 234),\n",
       " ('one', 215)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4.4\n",
    "#top 5 frequent words in non-spam documents\n",
    "\n",
    "emails = [os.path.join(directory,f) for f in os.listdir(directory)]\n",
    "keyword=\"spmsga\"\n",
    "all_words = []\n",
    "for email in emails:\n",
    "    if keyword not in email:\n",
    "        with open(email, 'r') as m: \n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2: \n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "dictionary3 = Counter(all_words)\n",
    "    \n",
    "    #extracting only words from the dictionary\n",
    "keys = dictionary3.keys()\n",
    "for item in all_words:\n",
    "    if item.isalpha() == False: \n",
    "        del dictionary3[item]\n",
    "    elif len(item) == 1:\n",
    "        del dictionary3[item]\n",
    "dictionary3 = dictionary3.most_common(5)\n",
    "dictionary3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[172, 289, 76, 29, 136, 253, 25, 58, 43, 135, 45, 217, 39, 34, 164, 29, 251, 2393, 204, 484, 33, 295, 25, 386, 667, 10, 958, 393, 53, 156, 895, 372, 42, 161, 208, 73, 444, 392, 89, 70, 31, 45, 48, 17, 684, 133, 231, 405, 116, 46, 271, 895, 604, 51, 215, 230, 666, 120, 144, 75, 61, 707, 92, 140, 177, 436, 32, 107, 68, 132, 51, 40, 187, 701, 113, 106, 240, 10, 31, 104, 11, 140, 43, 95, 58, 225, 68, 57, 92, 191, 875, 116, 123, 1618, 275, 339, 582, 471, 202, 398, 177, 97, 55, 106, 225, 47, 157, 577, 44, 153, 117, 80, 33, 95, 545, 114, 85, 13, 454, 80, 199, 51, 691, 22, 298, 40, 544, 562, 85, 456, 98, 309, 186, 205, 116, 178, 139, 7, 105, 471, 135, 75, 421, 115, 393, 181, 144, 200, 243, 398, 84, 77, 68, 29, 84, 30, 231, 124, 59, 79, 242, 54, 12, 124, 669, 464, 66, 327, 12, 51, 584, 43, 41, 124, 13, 362, 11, 317, 558, 68, 51, 48, 484, 72, 2715, 40, 99, 262, 106, 217, 68, 204, 322, 81, 469, 119, 111, 66, 846, 439, 730, 27, 316, 1008, 712, 206, 53, 183, 42, 248, 80, 191, 289, 104, 22, 96, 32, 312, 47, 350, 69, 152, 208, 158, 76, 972, 241, 42, 83, 75, 78, 11, 54, 265, 74, 148, 401, 370, 30, 220, 38, 204, 66, 38, 22, 895, 188, 91, 195, 211, 806, 811, 54, 104, 17, 1776, 148, 789, 93, 169, 118, 65, 222, 64, 200, 247, 538, 23, 154, 79, 149, 99, 83, 13, 127, 313, 63, 29, 331, 101, 117, 84, 117, 242, 41, 682, 21, 402, 101]\n",
      "Maximum no. of words in a document is:  2715\n",
      "Minimum no. of words in a document is:  7\n"
     ]
    }
   ],
   "source": [
    "#4.5\n",
    "#finding no. of words in a document\n",
    "emails = [os.path.join(directory,f) for f in os.listdir(directory)]\n",
    "all_words = []\n",
    "words_length = []\n",
    "for email in emails:\n",
    "    with open(email, 'r') as m: \n",
    "        for i,line in enumerate(m):\n",
    "            if i == 2: \n",
    "                words = line.split()\n",
    "                for item in words: #removing non-alphabets and punctuations\n",
    "                    if item.isalpha() == False: \n",
    "                        words.remove(item)\n",
    "                    elif len(item) == 1:\n",
    "                        words.remove(item)\n",
    "                words_length.append(len(words))\n",
    "                \n",
    "print(words_length)\n",
    "print(\"Maximum no. of words in a document is: \", max(words_length))\n",
    "print(\"Minimum no. of words in a document is: \", min(words_length))\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5. Create a feature extractor function similar to document_features in the NLTK example. Don’t copy the code from NLTK book. Use the feature extractor function to create a training dataset on Part 1 of the data. Train a Naive Bayes classifier as shown in the book chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.\n",
    "#Creating a dictionary of words in the emails\n",
    "\n",
    "dictionary = [] #globalizing variable\n",
    "\n",
    "def make_dictionary(train_dir):\n",
    "    emails = [os.path.join(train_dir,f) for f in os.listdir(train_dir)]    \n",
    "    all_words = []       \n",
    "    for mail in emails:    \n",
    "        with open(mail) as m:\n",
    "            for i,line in enumerate(m):\n",
    "                if i == 2:  #Body of email is 3rd line of text file\n",
    "                    words = line.split()\n",
    "                    all_words += words\n",
    "                    for item in all_words: #removing non-alphabets and punctuations\n",
    "                        if item.isalpha() == False: \n",
    "                            all_words.remove(item)\n",
    "                        elif len(item) == 1:\n",
    "                            all_words.remove(item)\n",
    "    \n",
    "    dictionary = Counter(all_words)\n",
    "    dictionary = dictionary.most_common(3000)\n",
    "\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#5.\n",
    "#features extractor - converts words in the emails to word count vectors (the feature).. \n",
    "#..and returns a full matrix of vectors\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def document_features(mail_dir): \n",
    "    emails = [os.path.join(mail_dir,f) for f in os.listdir(mail_dir)]\n",
    "    features_matrix = np.zeros((len(emails),3000))\n",
    "    doc_id = 0;\n",
    "    for email in emails:\n",
    "        with open(email) as f:\n",
    "            for i,line in enumerate(f):\n",
    "                if i == 2:\n",
    "                    words = line.split()\n",
    "                    for word in words:\n",
    "                        word_id = 0\n",
    "                        for i,d in enumerate(dictionary):\n",
    "                            if d[0] == word:\n",
    "                                word_id = i\n",
    "                                features_matrix[doc_id, word_id] = words.count(word)\n",
    "            doc_id = doc_id + 1     \n",
    "    return features_matrix\n",
    "\n",
    "mail_dir = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part1'\n",
    "all_arrays = document_features(mail_dir)\n",
    "print(all_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(289, 3000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Shape of the features_matrix\n",
    "all_arrays.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.\n",
    "#Preparing training data and labels\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Calling the make_dictionary function\n",
    "train_dir = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part1'\n",
    "dictionary = make_dictionary(train_dir)\n",
    "\n",
    "# Preparing feature vectors per training email and its labels\n",
    "\n",
    "train_labels = np.zeros(289)\n",
    "train_labels[241:289] = 1 #48 spam emails - file no. 241-289\n",
    "train_matrix = document_features(train_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5.\n",
    "# Training the Naive bayes classifier\n",
    "\n",
    "model1 = MultinomialNB()\n",
    "model1.fit(train_matrix,train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q6. For testing, we will use Part 10 in the lemm_stop folder. Follow similar steps as above to create a test dataset. Apply the feature extractor function to extract features from the test dataset. What is its accuracy on the test dataset? Show the code. What happens if you test on the training dataset? If you get accuracies below 50%, then, there is a bug in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8281786941580757\n",
      "0.9342560553633218\n"
     ]
    }
   ],
   "source": [
    "#6.\n",
    "# Testing the unseen emails for Spam\n",
    "\n",
    "test_dir = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part10'\n",
    "test_matrix = document_features(test_dir)\n",
    "test_labels = np.zeros(291)\n",
    "test_labels[242:291] = 1 #49 spam emails - files 242-291\n",
    "result1 = model1.predict(test_matrix)\n",
    "result_on_train_data = model1.predict(train_matrix)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(test_labels, result1))\n",
    "print(accuracy_score(train_labels, result_on_train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q7. Evaluation: What is the Precision, Recall, and F-score of the classifier that you trained? Read section 3 of the chapter to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "   spam_NaiveBayes       0.85      0.96      0.90       242\n",
      "nonspam_NaiveBayes       0.47      0.16      0.24        49\n",
      "\n",
      "          accuracy                           0.83       291\n",
      "         macro avg       0.66      0.56      0.57       291\n",
      "      weighted avg       0.79      0.83      0.79       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#7.\n",
    "#Metrics on the test data - Naive Bayes\n",
    "from sklearn import metrics\n",
    "target_labels = ['spam_NaiveBayes','nonspam_NaiveBayes']\n",
    "print(metrics.classification_report(test_labels,result1, target_names=target_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q8. Can you try another classifier such as Logistic Regression? How do the evaluation metrics look like? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shristidhakal/opt/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#8.\n",
    "#Training the logistic regression classifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model2 = LogisticRegression()\n",
    "model2.fit(train_matrix,train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#9.\n",
    "# Testing the unseen emails for Spam\n",
    "\n",
    "test_dir = '/Users/shristidhakal/Documents/Grad School/Comp Linguistics/week 9/lingspam_public/lemm_stop/part10'\n",
    "test_matrix = document_features(test_dir)\n",
    "test_labels = np.zeros(291)\n",
    "test_labels[242:291] = 1 #49 spam emails - files 242-291\n",
    "result2 = model2.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7250859106529209\n",
      "\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "   spam_LogisticReg       0.83      0.85      0.84       242\n",
      "nonspam_LogisticReg       0.14      0.12      0.13        49\n",
      "\n",
      "           accuracy                           0.73       291\n",
      "          macro avg       0.48      0.48      0.48       291\n",
      "       weighted avg       0.71      0.73      0.72       291\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#9. #Metrics on the test data - Logistic Regression\n",
    "print(accuracy_score(test_labels, result2))\n",
    "print('\\n')\n",
    "\n",
    "target_labels2 = ['spam_LogisticReg','nonspam_LogisticReg']\n",
    "print(metrics.classification_report(test_labels,result2, target_names=target_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
